{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Lab1_Artificial_Neural_Networks_in_Keras.ipynb","provenance":[{"file_id":"14fw9Ki4rDdFZX4dHjmnegL7rrkX2OFny","timestamp":1621471234085}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_4HB-98uzhvX"},"source":["Source: Adapted from Arden Dertat\n","# Table of contents\n","1. [Importing libraries](#Import-Libraries)\n","2. [Utility Functions](#Utility-Functions)\n","3. [Logistic Regression](#Logistic-Regression)\n","4. [Linearly-Separable Data](#Linearly-Data)\n","5. [Complex Data-Moons](#Moons-Data)\n","6. [Complex Data-Circles](#Circles-Data)\n","7. [Training a Deep Neural Network](#Deep-Net)\n","8. [Moons revisited](#Moons2)\n","9. [Circles revisited](#Circles2)\n","10. [Complex Data-Sine wave](#Sine-Wave)\n","11. [Multiclass classification](#MultiClass)\n","12. [SoftMax function](#SoftMax)\n","13. [Deep Learning for Multiclass classification](#Deep-Net2)"]},{"cell_type":"markdown","metadata":{"id":"JGWf0yeu0PUk"},"source":["# Importing libraries <a name=\"Import-Libraries\"></a>\n","Here we import some libraries needed later to run our code. \n","No need to go into details here, just run the cell."]},{"cell_type":"code","metadata":{"id":"FygebLAeWB4j"},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import numpy as np\n","import seaborn as sns\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","pd.options.display.float_format = '{:,.2f}'.format\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_columns', 200)\n","\n","from __future__ import print_function\n","from datetime import datetime\n","from matplotlib.colors import ListedColormap\n","from sklearn.datasets import make_classification, make_moons, make_circles\n","from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import shuffle\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, BatchNormalization, Activation\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import EarlyStopping\n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n","from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n","import keras.backend as K\n","from keras.wrappers.scikit_learn import KerasClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rX39AZ1DWB4o"},"source":["# Utility Functions <a name=\"Utility-Functions\"></a>"]},{"cell_type":"markdown","metadata":{"id":"RzvmZdwJWB4p"},"source":["These are the plotting helper functions used further down in the notebook. You don't need to fully understand what's going on here to get the big picture. The names of the functions are self-explanatory. I would at first recommend to skip this part and proceed to the next Logistic Regression section."]},{"cell_type":"code","metadata":{"id":"S3al46a0WB4p"},"source":["def plot_decision_boundary(func, X, y, figsize=(9, 6)):\n","    amin, bmin = X.min(axis=0) - 0.1\n","    amax, bmax = X.max(axis=0) + 0.1\n","    hticks = np.linspace(amin, amax, 101)\n","    vticks = np.linspace(bmin, bmax, 101)\n","    \n","    aa, bb = np.meshgrid(hticks, vticks)\n","    ab = np.c_[aa.ravel(), bb.ravel()]\n","    c = func(ab)\n","    cc = c.reshape(aa.shape)\n","\n","    cm = plt.cm.RdBu\n","    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","    \n","    fig, ax = plt.subplots(figsize=figsize)\n","    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n","    \n","    ax_c = fig.colorbar(contour)\n","    ax_c.set_label(\"$P(y = 1)$\")\n","    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n","    \n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n","    plt.xlim(amin, amax)\n","    plt.ylim(bmin, bmax)\n","\n","def plot_multiclass_decision_boundary(model, X, y):\n","    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n","    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n","    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n","\n","    Z = model.predict_classes(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n","    Z = Z.reshape(xx.shape)\n","    fig = plt.figure(figsize=(8, 8))\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n","    plt.xlim(xx.min(), xx.max())\n","    plt.ylim(yy.min(), yy.max())\n","    \n","def plot_data(X, y, figsize=None):\n","    if not figsize:\n","        figsize = (8, 6)\n","    plt.figure(figsize=figsize)\n","    plt.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n","    plt.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n","    plt.xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n","    plt.ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n","    plt.legend()\n","\n","def plot_loss_accuracy(history):\n","    historydf = pd.DataFrame(history.history, index=history.epoch)\n","    plt.figure(figsize=(8, 6))\n","    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n","    loss = history.history['loss'][-1]\n","    acc = history.history['accuracy'][-1]\n","    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n","\n","def plot_loss(history):\n","    historydf = pd.DataFrame(history.history, index=history.epoch)\n","    plt.figure(figsize=(8, 6))\n","    historydf.plot(ylim=(0, historydf.values.max()))\n","    plt.title('Loss: %.3f' % history.history['loss'][-1])\n","    \n","def plot_confusion_matrix(model, X, y):\n","    y_pred = model.predict_classes(X, verbose=0)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)\n","\n","def plot_compare_histories(history_list, name_list, plot_accuracy=True):\n","    dflist = []\n","    for history in history_list:\n","        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n","        dflist.append(pd.DataFrame(h, index=history.epoch))\n","\n","    historydf = pd.concat(dflist, axis=1)\n","\n","    metrics = dflist[0].columns\n","    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n","    historydf.columns = idx\n","    \n","    plt.figure(figsize=(6, 8))\n","\n","    ax = plt.subplot(211)\n","    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n","    plt.title(\"Loss\")\n","    \n","    if plot_accuracy:\n","        ax = plt.subplot(212)\n","        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n","        plt.title(\"Accuracy\")\n","        plt.xlabel(\"Epochs\")\n","\n","    plt.tight_layout()\n","    \n","def make_sine_wave():\n","    c = 3\n","    num = 2400\n","    step = num/(c*4)\n","    np.random.seed(0)\n","    x0 = np.linspace(-c*np.pi, c*np.pi, num)\n","    x1 = np.sin(x0)\n","    noise = np.random.normal(0, 0.1, num) + 0.1\n","    noise = np.sign(x1) * np.abs(noise)\n","    x1  = x1 + noise\n","    x0 = x0 + (np.asarray(range(num)) / step) * 0.3\n","    X = np.column_stack((x0, x1))\n","    y = np.asarray([int((i/step)%2==1) for i in range(len(x0))])\n","    return X, y\n","\n","def make_multiclass(N=500, D=2, K=3):\n","    \"\"\"\n","    N: number of points per class\n","    D: dimensionality\n","    K: number of classes\n","    \"\"\"\n","    np.random.seed(0)\n","    X = np.zeros((N*K, D))\n","    y = np.zeros(N*K)\n","    for j in range(K):\n","        ix = range(N*j, N*(j+1))\n","        # radius\n","        r = np.linspace(0.0,1,N)\n","        # theta\n","        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2\n","        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","        y[ix] = j\n","    fig = plt.figure(figsize=(6, 6))\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, alpha=0.8)\n","    plt.xlim([-1,1])\n","    plt.ylim([-1,1])\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pRilMYdWB4r"},"source":["# Logistic Regression <a name=\"Logistic-Regression\"></a>"]},{"cell_type":"markdown","metadata":{"id":"L8yDFIVnWB4s"},"source":["Despite its name, logistic regression (LR) is a binary *classification* algorithm. It's the most popular technique for 0/1 classification. LR will try to draw a straight line to separate the classes, that's where the term *linear model* comes from. LR works on arbitrary dimensions, not just two. For 3 dimensional data it'll try to draw a 2 dimensional plane to separate the classes. This generalizes to N dimensional data and N-1 dimensional hyperplane separator. If you have a supervised binary classification problem, given an input data with multiple columns and a binary 0/1 outcome, LR is the first method to try. In this section we will focus on 2 dimensional data since it's easier to visualize, and in the later sections we will work with a multidimensional input."]},{"cell_type":"markdown","metadata":{"id":"JyA2N1S1WB4t"},"source":["## Linearly Separable Data <a name=\"Linearly-Data\"></a>"]},{"cell_type":"markdown","metadata":{"id":"e0FKsRhSWB4t"},"source":["First let's start with the easy example. 2-dimensional linearly separable data. We are using the scikit-learn *make_classification* method to generate our data and use our helper function defined above to visualize it."]},{"cell_type":"code","metadata":{"id":"-RdGdTrLWB4u"},"source":["X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n","                           n_informative=2, random_state=7, n_clusters_per_class=1)\n","plot_data(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRf5-O8rWB4x"},"source":["There is a *LogisticRegression* classifier available in scikit-learn, I won't go into too much detail here since our goal is to learn building models with Keras. But here's how to train an LR model, using the *fit* function just like any other model in scikit-learn. We see the linear decision boundary as the green line."]},{"cell_type":"code","metadata":{"id":"Z7xRjutXWB4x"},"source":["lr = LogisticRegression()\n","lr.fit(X, y)\n","print('LR coefficients:', lr.coef_)\n","print('LR intercept:', lr.intercept_)\n","\n","plot_data(X, y)\n","\n","limits = np.array([-2, 2])\n","boundary = -(lr.coef_[0][0] * limits + lr.intercept_[0]) / lr.coef_[0][1]\n","plt.plot(limits, boundary, \"g-\", linewidth=2)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpRmYjMIWB40"},"source":["As we can see the data is linearly separable. We will train a logistic regression model with Keras to predict the class membership of every input point. To keep things simple for now, we won't perform the standard practices of separating out the data to training and test sets, or performing k-fold cross-validation. We will see examples of these in the later sections on real datasets. \n","\n","Keras has great documentation at https://keras.io/ so I won't go into every single detail, but will give a general overview.\n","\n","We will use the *Sequential* model API available [here](https://keras.io/getting-started/sequential-model-guide/). The Sequential model allows us to build deep neural networks, by stacking layers one on top of another. Since we're now building a simple logistic regression model, we will have the input nodes directly connected to output node, without any hidden layers.\n","\n","Quick clarification to disambiguate the terms being used. In neural networks literature, it's common to talk about input nodes and output nodes. This may sound strange at first glance, what's an input *node* per se? When we say input nodes, we're talking about the features of a given dataset. In our case we have 2 features, the x and y coordinates of the points we plotted above, so we have 2 input nodes. You can simply think of it as a vector of 2 numbers. What about the output node then? The output of the logistic regression model is a single number, the probability of an input data point belonging to class 1. In other words $P(class=1|X)$. The probability of an input point belonging to class 0 is then $P(class=0|X) = 1 - P(class=1|X)$. You can simply think of the output node as a vector with a single number between 0 and 1.\n","\n","In Keras we don't add layers corresponding to input nodes, we only do this for hidden nodes and output nodes. In our current model, we don't have any hidden layers, the input nodes are directly connected to the output node. This means our neural network definition in Keras will just have one layer with one node, corresponding to the output node.\n","\n","The *Dense* function in Keras constructs a fully connected neural network layer, automatically initializing the weights and biases randomly. It's a super useful function that you will see everywhere. The function arguments are defined as follows:\n","- *units*: The first argument, the number of nodes in this layer. Since we're constructing the output layer, and we said it has only one node, this value is 1.\n","- *input_shape*: We need to specify the input dimensions for the first layer in Keras models. We don't need to specify this argument for  the subsequent layers (which we don't have here but we will in later sections) because Keras can infer their dimensions automatically. In this case, our input dimensionality is 2, the x and y coordinates. The input_shape parameter expects a vector, so in our case it's simply a tuple with one number.\n","- *activation*: The activation function of a logistic regression model is the *logistic* function, or altenatively called as *sigmoid*. \n","\n","We then compile the Keras model with the *compile* function. This creates the neural network model by configuring the learning process. The model hasn't been trained yet. Right now we're specifying the optimizer to use and the loss function to minimize. Don't worry for now about these parameters as we will study it later in details. The arguments for the compile function are defined as follows:\n","- *optimizer*: Which optimizer to use in order to minimize the loss function. There are a lot of different optimizers, most of them based on gradient descent.  For now we will use the *adam* optimizer, which is the one community prefer to use by default.\n","\n","- *loss*: The loss function to minimize. Since we're building a binary 0/1 classifier, the loss function to minimize is *binary_crossentropy*. We will see other examples of loss functions in later sections.\n","- *metrics*: Which metric to report statistics on, for classification problems we set this as *accuracy*.\n","\n","Now comes the fun part of actually training the model. The arguments are as follows:\n","- *x*: The input data, we defined it as *X* above. It contains the x and y coordinates of the input points\n","- *y*: Not to be confused with the y coordinate of the input points. The class we're trying to predict: 0 or 1.\n","- *verbose*: Prints out the loss and accuracy, set it to 1 to see the output.\n","- *epochs*: Number of times to go over the training data. When training models we pass through the training data not just once but multiple times.\n","\n","The output of the fit method is the loss and accuracy at every epoch. We then plot it to see that the loss goes down to almost 0 over time, and the accuracy goes up to almost 1. Great! We have successfully trained our first neural network model with Keras. I know this was a long explanation, but I wanted to explain what we're doing in detail the first time. Once you understand what's going on and practice a couple of times, all this becomes second nature."]},{"cell_type":"code","metadata":{"id":"U3kr15-wWB40"},"source":["model = Sequential()\n","model.add(Dense(units=1, input_shape=(2,), activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(x=X, y=y, verbose=0, epochs=50)\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn5GvkxlWB43"},"source":["A fancy plot of the decision boundary. The various shades of blue and red represent the probability of a hypothetical point in that area belonging to class 1. The top left area is classified as class 1, so the dark blue. The bottom right area is classified as class 0, so the dark red. And there is a transition around the decision boundary. It's a nice way to visualize the separator function the model is learning."]},{"cell_type":"code","metadata":{"id":"ig-U7m-NWB43"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PGd3Z-6EWB46"},"source":["The classification report showing the precision and recall of our model."]},{"cell_type":"code","metadata":{"id":"wnxlLi12WB47"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CDkeU1LAWB4_"},"source":["The confusion matrix shows us how many classes were correctly classified vs misclassified. The numbers on the diagonal axis represent the number of correctly classified points, the rest are the misclassified ones. This particular matrix is not very interesting because the model correctly predicts the class of all but one point. We can see the misclassified point at the top right part of the confusion matrix, true value is class 0 but the predicted value is class 1."]},{"cell_type":"code","metadata":{"id":"dyJFRC8DWB5A"},"source":["plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kC6dpwe0WB5D"},"source":["## Complex Data - Moons <a name=\"Moons-Data\"></a>"]},{"cell_type":"markdown","metadata":{"id":"8FeVMw4HWB5D"},"source":["The previous dataset was linearly separable, so it was trivial for our logistic regression model to separate the classes. Here is a more complex dataset which isn't linearly separable. The simple logistic regression model won't be able to distinguish between the classes."]},{"cell_type":"code","metadata":{"id":"yOU6GHyuWB5E"},"source":["X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)\n","plot_data(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJrWO8xmWB5J"},"source":["We're building  another logistic regression model with the same parameters as we did before."]},{"cell_type":"code","metadata":{"id":"ShCNTd-VWB5J"},"source":["model = Sequential()\n","model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=100)\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e9rHaDVpWB5M"},"source":["The current decision boundary doesn't look as clean as the one before. The model tried to separate out the classes from the middle, but there are a lot of misclassified points. We need a more complex classifier with a non-linear decision boundary, and we will see an example of that soon."]},{"cell_type":"code","metadata":{"id":"S8pCKB_AWB5M"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeG6a1JxWB5Q"},"source":["Precision of the model is 86%. It looks good on paper but we should be able to get 100% with a more complex model. You can imagine a curved decision boundary that will separate out the classes, and a complex model should be able to approximate that."]},{"cell_type":"code","metadata":{"id":"LXBIzOYIWB5Q"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6TnYxxNWB5U"},"source":["## Complex Data - Circles <a name=\"Circles-Data\"></a>"]},{"cell_type":"markdown","metadata":{"id":"V4UdRdjbWB5V"},"source":["Let's look at one final example where the liner model will fail."]},{"cell_type":"code","metadata":{"id":"h4fT1HioWB5W"},"source":["X, y = make_circles(n_samples=1000, noise=0.05, factor=0.3, random_state=0)\n","plot_data(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_V3HUOVxWB5Z"},"source":["Building the model with same parameters."]},{"cell_type":"code","metadata":{"id":"0x1UaSNUWB5a"},"source":["model = Sequential()\n","model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=100)\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Wi2q3_1WB5d"},"source":["The decision boundary again passes from the middle of the data, but now we have much more misclassified points."]},{"cell_type":"code","metadata":{"id":"FN9KrW6_WB5d"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YhX3o3VWB5h"},"source":["The accuracy is 51%. No matter where the model draws the line, it will misclassify half of the points, due to the nature of the dataset.\n","\n","The confusion matrix we see here is an example of a poor classifier. Ideally, we want confusion matrices like the ones we saw above. High numbers along the diagonals meaning that the classifier was right, and low numbers everywhere else where the classifier was wrong. In our visualization, the color blue represents the high numbers and yellow represents the low ones. So we would prefer to see blue on the diagonals and yellow everywhere else, blues everywhere is a bad sign meaning that our classifier is confused."]},{"cell_type":"code","metadata":{"id":"M_It47J0WB5i"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"1pAaOO-IWB5k"},"source":["# Training a Deep Neural Network <a name=\"Deep-Net\"></a>"]},{"cell_type":"markdown","metadata":{"id":"GQYSLh98WB5k"},"source":["Now we will train a deep Artificial Neural Network (ANN) to better classify the datasets which the logistic regression model struggled, Moons and Circles datasets. We will also classify an even harder dataset of Sine Wave to demonstrate that ANNs can form really complex decision boundaries."]},{"cell_type":"markdown","metadata":{"id":"OgNOMi8gWB5l"},"source":["## Complex Data - Moons <a name=\"Moons2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"-OBv5NNyWB5l"},"source":["While building Keras models for logistic regression above, we performed the following steps:\n","- Step 1: Define a Sequential model.</p>\n","- Step 2: Add a Dense layer with sigmoid activation function. This was the only layer we needed.</p>\n","- Step 3: Compile the model with an optimizer and loss function.</p>\n","- Step 4: Fit the model to the dataset.</p>\n","- Step 5: Analyze the results: plotting loss/accuracy curves, plotting the decision boundary, looking at the classification report, and understanding the confusion matrix.</p>\n","\n","While building a deep neural network, we only need to change step 2 such that, we will add several Dense layers one after another. The output of one layer becomes the input of the next. Keras again does most of the heavylifting of initializing the weights and biases, and connecting the output of one layer to the input of the next. We only need to specify how many nodes we want in that layer, and the activation function. It's as simple as that.\n","\n","As you can see, we first add a layer with 4 nodes and *tanh* activation function. Tanh is a commonly used activation function. We then add another layer with 2 nodes and again tanh activation. We finally add the last layer with 1 node and sigmoid activation. This is the final layer that we also used in the logistic regression model.\n","\n","This is not a very deep ANN, it only has 3 layers: input layer, 1 hidden layer, and the output layer. But notice a couple of patterns:\n","- Output layer still uses the sigmoid activation function since we're working on a binary classification problem.\n","- Non-output layers use the tanh activation function. \n","- We have fewer number of nodes in each subsequent layer. It's common to have less nodes as we stack layers on top of one another.\n","\n","We didn't build a very deep ANN here because it wasn't necessary. We already achieve perfect accuracy with this configuration."]},{"cell_type":"code","metadata":{"id":"qvXXowW_WB5m"},"source":["X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)\n","\n","model = Sequential()\n","model.add(Dense(4, input_shape=(2,), activation='tanh'))\n","model.add(Dense(2, activation='tanh'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(Adam(lr=0.01), 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=100)\n","\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DmWjiaSWB5o"},"source":["The ANN is able to come up with a perfect separator to distinguish the classes."]},{"cell_type":"code","metadata":{"id":"QiEsVe7tWB5p"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4d99EldWB5v"},"source":["100% precision, nothing misclassified."]},{"cell_type":"code","metadata":{"id":"QeRTSeW_WB5w"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji_mPy9GWB50"},"source":["## Complex Data - Circles <a name=\"Circles2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"uX4-fW7UWB51"},"source":["Now let's look at the Circles dataset, where the LR model achieved only 50% accuracy. The model is the same as above, we only change the input to the fit function. And we again achieve 100% accuracy."]},{"cell_type":"code","metadata":{"id":"RsWUmyIlWB52"},"source":["X, y = make_circles(n_samples=1000, noise=0.05, factor=0.3, random_state=0)\n","\n","model = Sequential()\n","model.add(Dense(4, input_shape=(2,), activation='tanh'))\n","model.add(Dense(2, activation='tanh'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(Adam(lr=0.01), 'binary_crossentropy', metrics=['accuracy'])\n","#model.compile(SGD(lr=0.001), 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=100)\n","\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTYFbs4UWB55"},"source":["Similarly the decision boundary looks just like one we would draw by hand ourselves. The ANN was able to figure out an optimal separator."]},{"cell_type":"code","metadata":{"id":"U6KwFNUHWB56"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oq3wC_d8WB59"},"source":["Just like above we get 100% accuracy."]},{"cell_type":"code","metadata":{"id":"8ThWZDAJWB59"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1n31Xow8WB6A"},"source":["## Complex Data - Sine Wave <a name=\"Sine-Wave\"></a>"]},{"cell_type":"markdown","metadata":{"id":"sLZ7BooWWB6B"},"source":["Let's try to classify one final toy dataset, and then we will move on to real-world examples. In the previous sections, the classes were separable by one continuous decision boundary. The boundary had a complex shape, it wasn't linear, but still one decision boundary was enough. ANNs can draw arbitrary number of complex decision boundaries, and we will demonstrate that.\n","\n","Let's create a sinusoidal dataset looking like the sine function, every up and down belongs to an alternating class. As we can see in the figure, a single decision boundary won't be able to separate out the classes. We will need a series of non-linear boundaries."]},{"cell_type":"code","metadata":{"id":"Y9-TN4XMWB6B"},"source":["X, y = make_sine_wave()\n","\n","plot_data(X, y, figsize=(10, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QrNU_B5WB6D"},"source":["Now we need a more complex model for accurate classification. So we have an input layer, 2 hidden layers, and an output layer. The number of nodes per layer has also increased to improve the learning capacity of the model. Choosing the right number of hidden layers and nodes per layer is usually decided by trial and error. However, there are methods to learn architecture automatically. "]},{"cell_type":"code","metadata":{"id":"34-rae-8WB6E"},"source":["model = Sequential()\n","model.add(Dense(64, input_shape=(2,), activation='tanh'))\n","model.add(Dense(64, activation='tanh'))\n","model.add(Dense(64, activation='tanh'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=50)\n","\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNKTk1bTWB6G"},"source":["The ANN was able to model a pretty complex set of decision boundaries."]},{"cell_type":"code","metadata":{"id":"r5kmJP3SWB6G"},"source":["plot_decision_boundary(lambda x: model.predict(x), X, y, figsize=(12, 9))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40vxmM-aWB6J"},"source":["Precision is 99%, we have 14 misclassified points out of 2400."]},{"cell_type":"code","metadata":{"id":"BBu2QoXQWB6K"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fh6Ax2AWB6M"},"source":["# Multiclass Classification <a name=\"MultiClass\"></a>"]},{"cell_type":"markdown","metadata":{"id":"799i929lWB6N"},"source":["In the previous sections we worked on a binary classification problem. Now we will take a look at a multi-class classification problem, where the number of classes is more than 2. We will pick 3 classes for demonstration, but our approach generalizes to any number of classes.\n","\n","Here's how our dataset looks like, spiral data with 3 classes."]},{"cell_type":"code","metadata":{"id":"XIhFyTQgWB6S"},"source":["X, y = make_multiclass(K=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waAvhC2nWB6U"},"source":["## Softmax Regression <a name=\"SoftMax\"></a>"]},{"cell_type":"markdown","metadata":{"id":"nbJPsMD1WB6U"},"source":["As we saw above, Logistic Regression (LR) is a classification method for 2 classes. It works with binary labels 0/1. Softmax Regression (SR) is a generalization of LR where we can have more than 2 classes. In our current dataset we have 3 classes, represented as 0/1/2.\n","\n","Building the model for SR is very similar to LR, for reference here's how we built our Logistic Regression model."]},{"cell_type":"code","metadata":{"id":"O0HwXxXGWB6V"},"source":["model = Sequential()\n","model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(X, y, verbose=0, epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ub8zu41aWB6Y"},"source":["And here's how how we will build the Softmax Regression model. There are a couple of differences, let's go over them one by one:\n","- Number of node in the **final** dense layer: LR uses 1 node, where SR has 3 nodes. Since we have 3 classes it makes sense for SR to have 3 nodes. Then the question is, why does LR uses only 1 node, it has 2 classes so it appears like we should have used 2 nodes instead. The answer is, because we can achieve the same result with using only 1 node. As we saw above, LR models the probability of an example belonging to class one: $P(class=1|X)$. And we can calculate class 0 probability by: $1 - P(class=1|X)$. But when we have more than 2 classes, we need individual nodes for each class. Because knowing the probability of one class doesn't let us infer the probability of the other classes.\n","- Activation function: LR used sigmoid activation function, SR uses *softmax*. It scales the probability of the output neurons such that they sum up to 1. So in our case $P(class=0|X) + P(class=1|X) + P(class=2|X) = 1$. It doesn't do it in a naive way by dividing individual probabilities by the sum though, it uses the exponential function. So, the higher values get emphasized more and lower values get \"squashed\".  We seen it in the lecture. You can simply think of it as a normalization function which let's us interpret the output values as probabilities.\n","- Loss function: In a binary classification problem like LR, the loss function is binary_crossentropy. In the multiclass case like SR, the loss function is categorical_crossentropy. Going into the theory behind loss functions is beyond the scope of this tutorial. But for now only knowing this property is enough.\n","- Fit function: LR used the vector y directly in the fit function, which has just one column with binary 0/1 values. When we're doing SR the labels need to be in *one-hot* representation. In our case y_cat is a matrix with 3 columns, where all the values are 0 except for the one that represents our class, and that is 1.\n","\n","It took a while to talk about all the differences between LR and SR, and it looks like as if there's a lot to learn. But again after some practice this will become a habit, and you won't even need to think about any of this.\n","\n","After all this theory let's take a step back and remember that LR is a linear classifier. SR is also a linear classifier, but for multiple classes. So the \"power\" of the model hasn't changed, it's still a linear model. We just generalized LR to apply it to a multiclass problem.\n","\n","Training the model gives us an accuracy of around 50%. The simplest possible technique which always predicts class 0 no matter what the input is would have an accuracy of 33%. This simple SR model is not much of an improvement over it, because the dataset is not linearly separable."]},{"cell_type":"code","metadata":{"id":"2JNf7NCKWB6Z"},"source":["model = Sequential()\n","model.add(Dense(3, input_shape=(2,), activation='softmax'))\n","\n","model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n","\n","y_cat = to_categorical(y)\n","history = model.fit(X, y_cat, verbose=0, epochs=20)\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIkypWhBWB6b"},"source":["Looking at the decision boundary confirms that we still have a linear classifier. The lines look jagged due to floating point rounding but in reality they're straight."]},{"cell_type":"code","metadata":{"id":"dnIHE4f4WB6c"},"source":["plot_multiclass_decision_boundary(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICn4UsnyWB6h"},"source":["We see the precision and recall corresponding to the 3 classes. And the confusion matrix is all over the place. Clearly this is not an optimal classifier."]},{"cell_type":"code","metadata":{"id":"1md2Nm4qWB6i"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0-WzI8SWB6k"},"source":["## Deep Neural Network <a name=\"Deep-Net2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"c0BNLUXkWB6k"},"source":["Now let's build a deep ANN for multiclass classification. Remember that the changes going from LR to deep ANN were minimal. We only needed to add more Dense layers. We'll do the same thing again. Adding a couple of Dense layers with tanh activation function, and decreasing the number of nodes per layer.\n","\n","Notice, that the output layer still has 3 nodes, and uses the softmax activation. The loss function also didn't change, still categorical_crossentropy. These won't change going from a linear model to a deep ANN, since the problem definition hasn't changed. We're still working on multiclass classification. We're now using a more powerful model, and that power comes from adding more layers to our neural network.\n","\n","We achieve 99% in just a couple of epochs."]},{"cell_type":"code","metadata":{"id":"ikjaOgbiWB6m"},"source":["model = Sequential()\n","model.add(Dense(128, input_shape=(2,), activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n","\n","y_cat = to_categorical(y)\n","history = model.fit(X, y_cat, verbose=0, epochs=50)\n","plot_loss_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hune6raWB6p"},"source":["The decision boundary is non-linear."]},{"cell_type":"code","metadata":{"id":"4lmpjM_sWB6q"},"source":["plot_multiclass_decision_boundary(model, X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ll7ZlFlTWB6s"},"source":["We got almost 100% accuracy. Weâ€™ve totally misclassified 6 points out of 1500."]},{"cell_type":"code","metadata":{"id":"oyyjsndYWB6s"},"source":["y_pred = model.predict_classes(X, verbose=0)\n","print(classification_report(y, y_pred))\n","plot_confusion_matrix(model, X, y)"],"execution_count":null,"outputs":[]}]}