{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STAT448_Introduction_to_ConvNets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbZdzl8C6chT",
        "colab_type": "text"
      },
      "source": [
        "Authors: Varvara Vetrova, Sheldon Coup (Msc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEg4t2xy6iqL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The following part is just additional libraries and functions that we neeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkJsIyAgePUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "from __future__ import print_function\n",
        "from datetime import datetime\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_classification, make_moons, make_circles\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
        "import keras.backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2Eyy4cesXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_loss_accuracy(history):\n",
        "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
        "    loss = history.history['loss'][-1]\n",
        "    acc = history.history['accuracy'][-1]\n",
        "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSwfjEv5u849",
        "colab_type": "text"
      },
      "source": [
        "**NOTE : In order to run this notebook quickly we will need to enable GPU acceleration.**\n",
        "\n",
        "**In the top menu go to Edit -> Notebook Settings -> Hardware Acceleration and click on GPU**\n",
        "\n",
        "If you do not do this, training of the neural networks will take **much** longer ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBVuodQ9eUcX",
        "colab_type": "text"
      },
      "source": [
        "As we seen, fully connected neural networks are powerful tools for vectorized data. By converting images to vectors, we can use fully connected networks to classify images, as we did with MNIST. However, when we have larger images and colour images, the number of parameters quickly gets out of hand.\n",
        "\n",
        "Lets download the Cifar10 dataset and have a look at it. \\\n",
        "[More information on Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbhs8JLyfSRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# converting the data type and scaling between 0 and 1\n",
        "x_train = x_train.astype('float32')/255.\n",
        "x_test = x_test.astype('float32')/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNhJTynfdby",
        "colab_type": "text"
      },
      "source": [
        "Lets have a look at the dimensions of the cifar images and check out the first image in the dataset (it's a frog =))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_q_X_yUfms2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Training data shape: \", x_train.shape) # (60000, 28, 28) -- 50000 images, each 32x32 pixels, 3 colours\n",
        "print(\"Test data shape\", x_test.shape) # (10000, 28, 28) -- 10000 images, each 32x32, 3 colours\n",
        "print(\"Training response shape:, \", y_train.shape)\n",
        "print(\"Testing response shape: \", y_test.shape)\n",
        "\n",
        "image_size = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
        "print(image_size)\n",
        "\n",
        "plt.imshow(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPaMgx56f7x7",
        "colab_type": "text"
      },
      "source": [
        "From this we can see that we have images that are 32 * 32 * 3. This means that we have a 32 * 32 image with 3 different colour channels (RGB).\n",
        "If we convert this to a vector we will have a vector of length 32 * 32 * 3 = 3072, which is significanlty larger than what we had to deal with for MNIST. Do you remember dimensions for MNIST?\n",
        "\n",
        "Let's reshape (flatten) these arrays. The images will be converted to vectors. However, unlike last time, we aren't going to overwrite the x_train and x_test objects, we will keep them around for later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrUO5VfagkiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the images\n",
        "image_vector_size = image_size[0] * image_size[1] * image_size[2]\n",
        "\n",
        "\n",
        "x_train_vector = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "\n",
        "x_test_vector = x_test.reshape(x_test.shape[0], image_vector_size)\n",
        "\n",
        "print(x_train_vector.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHWsAcFPh4XU",
        "colab_type": "text"
      },
      "source": [
        "Before moving onto building the network we need to convert the labels into one-hot-encoded target labels. Like last time we are using the to_categorical function to do this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WqsxC8JgoYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"First 5 training labels: \", y_train[:5]) \n",
        "\n",
        "# Convert to \"one-hot\" vectors using the to_categorical function\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print(\"First 5 training lables as one-hot encoded vectors:\\n\", y_train[:5])\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oAPx7_OiYyy",
        "colab_type": "text"
      },
      "source": [
        "Firstrly, let's build a simple fully connected model similar to one in the lab last week, but with two hidden layers this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOg_7-_siDLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_basic_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  # The input layer requires the special input_shape parameter which should match\n",
        "  # the shape of our training data.\n",
        "  model.add(Dense(units=1024, activation='sigmoid', input_shape=(image_vector_size,)))\n",
        "  model.add(Dense(units=256, activation='sigmoid'))\n",
        "  model.add(Dense(units=num_classes, activation='softmax'))\n",
        "  return model\n",
        "model = build_basic_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWxes3AikhT",
        "colab_type": "text"
      },
      "source": [
        "With nearly **3.5 million** parameters to learn, our model is much bigger than last time.\n",
        "Let's compile the model with an ADAM optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJdv5LbxiWsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKoPXFpSi9pR",
        "colab_type": "text"
      },
      "source": [
        "Learning from the last lab, we will train for 50 epochs the first time around. Please note it might take a while but not too long. If takes ages, please check the GPU settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M-F6Xrfi5tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train_vector, y_train, batch_size=512, epochs=50, verbose=True, validation_split=.1)\n",
        "loss, accuracy  = model.evaluate(x_test_vector, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLUJCxav9sG",
        "colab_type": "text"
      },
      "source": [
        "That seems to be learning well, lets have a look at the loss /accuracy curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJSuSCUejHtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBmG9JFzwDVA",
        "colab_type": "text"
      },
      "source": [
        "In the early stage of training all is going well, but the model is **not **generalizing well towards the end! This can be seen by the **widening gap between the validation and training loss** and accuracy lines. This is a sign that the network is *overfitting*.\n",
        "\n",
        "This overfitting may be related to the fact that we have over 3.5 million parameters in our model. Imagine if we had a larger input image, the number of parameters would explode!\n",
        "If we had a 300 * 300 RBG image then we would have over 300 million parameters for our little 2 layer network. \n",
        "\n",
        "Finally, our vectorized data and the fully connected model doesn't take into account any of the spatial relationships in the image, as its just jumbles all the pixels around into a single vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYL5srQJjoX1",
        "colab_type": "text"
      },
      "source": [
        "For these reasons we would like to move on to a *Convolutional Neural Network*.\n",
        "\n",
        "Let's write a function to put together a convolutional neural network. We will be using a new layer called **Conv2D**.\\\n",
        "The Conv2D layer takes in the following arguments:\n",
        "\n",
        "\n",
        "1.   Number  of filters, this is roughly similar to setting the number of neurons in a Dense layer. Must be an integer.\n",
        "2.   Filter size, the dimension of each filter being used. Tuple of integers\n",
        "3.   Activation, similar sort of functions as we used for Dense layers.\n",
        "4.   Padding, this determines if the model adds extra rows /columns to the input. This gives control over the shape of the output. For more details refer to the lecture slides, Lecture on Convolutioan nerual networks on Learn.\n",
        "\n",
        "\n",
        "More information on convolutional layers can be found [here](https://keras.io/layers/convolutional/).\n",
        "\n",
        "One thing to note about convolutional neural networks is that they still normally have the Dense layers in the end like a usual fully-connected network.\n",
        "\n",
        "Below is a function that puts together a simple convolutional network:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsebqNTaj_1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_convnet():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape = image_size))\n",
        "  model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation = 'relu', padding='same'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  \n",
        " \n",
        "  return model\n",
        "model = build_convnet()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptY8rER7FdRs",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our network and use Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNIHPIznpfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XrHj2HV2DsN",
        "colab_type": "text"
      },
      "source": [
        "**Reminder: If you have not enabled GPU hardware acceleration yet then training this convnet will take much longer!!!.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq168hyUoBBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=256, epochs=50, verbose=True, validation_split=.1, shuffle=True)\n",
        "loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN7LaK1moGJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osez6rkxyX0B",
        "colab_type": "text"
      },
      "source": [
        "We can see here that the model is doing better than the fully connected one  but it started to overfit again towards the end. \n",
        "Let's add some Dropout layers to try and combat this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnQ55SdxyeXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dropout_convnet():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape = image_size))\n",
        "  model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation = 'relu', padding='same'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  \n",
        " \n",
        "  return model\n",
        "model = build_dropout_convnet()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgEqiU26ywTU",
        "colab_type": "text"
      },
      "source": [
        "Lets compile and train the model for the same number of epochs as before.\n",
        "\n",
        "**Please note that if you would like to train the same model again you need to also compile it again. If you just call model.fit function your weights will be initiliased from the last time you trained that model!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESZLH3kJynMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train.astype('float32'), y_train, batch_size=512, epochs=50, verbose=True, validation_split=.1, shuffle=True)\n",
        "loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXFk0TWy4Bn",
        "colab_type": "text"
      },
      "source": [
        "Hopefully, thats a bit better, lets have a look at the loss / accuracy curves and check the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmAHzSiUymWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjW16GzHzB_H",
        "colab_type": "text"
      },
      "source": [
        "Great! The convolutional model works. Clearly 50 epochs isn't quite enough, but we can see how much the dropout layers have improved the generalization performance of the model.\n",
        "\n",
        "However, to help us get an idea of whats actually going on lets look at how the layers respond to a particular image. In other words, let's look at **feature maps**!\n",
        "\n",
        "First, let's look at an image that we are going to use. Feel free to change this index to whatever number you like, so you can see how it works with different images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4hgxkPF0E8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo_image_index = 0\n",
        "\n",
        "plt.imshow(x_train[demo_image_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZyfUo-Y0bKj",
        "colab_type": "text"
      },
      "source": [
        "Lets see how one of the first filters is responding to this image. If we plot single layer of this feature map we can see that is will be picking up specific features of the input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Db8q8MvlWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_outputs = [layer.output for layer in model.layers[:8]] \n",
        "# Extracts the outputs of the top several layers\n",
        "\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
        "activations = activation_model.predict(np.expand_dims(x_train[demo_image_index], axis=0))\n",
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)\n",
        "\n",
        "#change this index and re-run this cell to have a look at different layers, see if you can figure out what the layers are responding\n",
        "demo_filter_index = 2\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, demo_filter_index], cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H73HavWe0_wu",
        "colab_type": "text"
      },
      "source": [
        "Lets plot all the activation maps, don't worry about how the code works in this section, it is just for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-RZMEXqI8Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use this code to free up some RAM\n",
        "import gc\n",
        "gc.collect()\n",
        "del(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlgYLKLuK-1E",
        "colab_type": "text"
      },
      "source": [
        "So, what you will see are the results of applying convolutional and maxpooling operations to the input image above. For instance, first 32 images are the result of applying 32 filters in the first convolutional layer to our input. \n",
        "See how the feature maps don't change in MaxPooling layer, however, they become more pixalated due to the reduction in size. \n",
        "Interesting thing is that some feature maps are zeros. It can happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvt3ENjxbSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_names = []\n",
        "for layer in model.layers[:6]:\n",
        "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
        "    \n",
        "images_per_row = 16\n",
        "\n",
        "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
        "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
        "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
        "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Jeqi5IODJ1",
        "colab_type": "text"
      },
      "source": [
        "**Feel free to experiment with this network, try to change settings, observe how it effects the perfomance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSvN706s5QYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}