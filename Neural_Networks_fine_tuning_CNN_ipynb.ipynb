{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_fine_tuning_CNN_ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKrQKjc5ANjJ",
        "colab_type": "text"
      },
      "source": [
        "In this lab we are going to perform fine-tuning and feature transfer for the dataset called Oxford Flowers 102. \n",
        "We will see how we can use large CNNs (VGG-16, Inception and MobileNet) already pre-trained by others on ImageNet on this dataset. We will compare results of ImageNet pretraining with just using random weights for the filters. In addition, we will perform feature transfer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwgo5e3J_7vQ",
        "colab_type": "text"
      },
      "source": [
        "##Importing libaries\n",
        "Have a look especially at Keras applications [Keras applications help](https://keras.io/api/applications/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwETEphRkvkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#General imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "from __future__ import print_function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Keras imports\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "# application (model) imports\n",
        "from keras import applications\n",
        "#from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhX5Z4hQBKmY",
        "colab_type": "text"
      },
      "source": [
        "We created a new function ***network_library*** below. It will allow us to work with commonly used architectures of CNNs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqcMxzYylF5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_loss_accuracy(history):\n",
        "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
        "    loss = history.history['loss'][-1]\n",
        "    acc = history.history['accuracy'][-1]\n",
        "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
        "\n",
        "def dir_to_array(dir, image_shape, verbose=1): #this functions helps us to convert all images from a directory into a Numpy array. It also changes the size of the images.\n",
        "  data = []\n",
        "  labels = []\n",
        "  image_shape = (image_shape[0], image_shape[1])\n",
        "  if verbose ==1:\n",
        "    bar = progressbar.ProgressBar(maxval=len(os.listdir(dir)),\n",
        "                                  widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    i = 0\n",
        "  for classname in os.listdir(dir):\n",
        "    class_dir = os.path.join(dir, classname)\n",
        "    for imagename in os.listdir(class_dir):\n",
        "      data.append(np.array(load_img(os.path.join(class_dir, imagename), target_size=image_shape )))\n",
        "      labels.append(int(classname)-1)\n",
        "    if verbose == 1:\n",
        "      i +=1\n",
        "      bar.update(i)\n",
        "  if verbose == 1:\n",
        "    bar.finish()\n",
        "  data = np.stack(data, axis=0)\n",
        "  labels = np.expand_dims(np.stack(labels, axis=0), axis=1)\n",
        "  return data, labels\n",
        "\n",
        "\n",
        "def network_library(network_name, num_classes, weights='imagenet'): #Heavily uses example code from Keras \n",
        "  # Function to build keras applications \n",
        "  if network_name == 'vgg16':\n",
        "    \n",
        "    input_shape = (224,224,3)\n",
        "    base_model = applications.vgg16.VGG16(include_top=False, weights=weights, input_shape=input_shape, pooling='max') #keep only convolutional and pooling layers, populate weights with pre-trained on ImageNet values\n",
        "    preprocess_input = applications.vgg16.preprocess_input #function which helps with preprocessing of input images\n",
        "    \n",
        "    x = base_model.output\n",
        "    x = Dense(4096, activation='relu')(x) #Add couple of dense layers on top\n",
        "    x=Dropout(0.5)(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x) #Last layer does classification into num_classes categories\n",
        "    \n",
        "    model = Model(inputs=base_model.input, outputs=predictions) # Model altogether\n",
        "    \n",
        "  elif network_name == 'inceptionv3':\n",
        "    \n",
        "    input_shape = (299,299,3)\n",
        "    base_model = applications.inception_v3.InceptionV3(include_top = False, weights=weights, input_shape=input_shape, pooling='avg')\n",
        "    preprocess_input = applications.inception_v3.preprocess_input\n",
        "    \n",
        "    x = base_model.output\n",
        "    x = Dense(1024, activation='relu')(x) #Keras functional API\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model  = Model(inputs = base_model.inputs, outputs=predictions)\n",
        "  elif network_name == 'mobilenetv2':\n",
        "    \n",
        "    input_shape = (224,224,3)\n",
        "    base_model = applications.mobilenet_v2.MobileNetV2(input_shape=input_shape, alpha=1.0, include_top=False, weights=weights, pooling='avg')\n",
        "    preprocess_input = applications.mobilenet_v2.preprocess_input\n",
        "    \n",
        "    x = base_model.output\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs = base_model.input, outputs=predictions)\n",
        "  else:\n",
        "    raise ValueError('{} is not a supported network'.format(network_name))\n",
        "  return model, input_shape, preprocess_input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm-lDzot_2xe",
        "colab_type": "text"
      },
      "source": [
        "As we said before, for this lab we are going to have a look at a range of different ways of performing a classification on images using a convolutional neural network. We will be looking at the Flowers 102 dataset, a tricky dataset that is used very commonly in computer vision.\n",
        "\n",
        "Let's download and unzip the data. The ! points here are telling the notebooks to run the commands in bash/terminal, these are not python commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6_ydRhXnqCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -r flower_data\n",
        "! rm flower_data.zip\n",
        "!wget \"https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\" \n",
        "!unzip -q flower_data.zip\n",
        "!ls \n",
        "! ls flower_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np2zsjW7tvKy",
        "colab_type": "text"
      },
      "source": [
        "Now that we have downloaded the data, lets set the data paths so that we can find the correct data folders.\n",
        "\n",
        "The last line is just checking that the we have the same number of test classes as we have training classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHroTSbomkKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the data paths\n",
        "train_dir = 'flower_data/train/'\n",
        "test_dir = 'flower_data/valid'\n",
        "\n",
        "# find the number of classes\n",
        "num_classes = len(os.listdir(train_dir))\n",
        "\n",
        "# check that the training and testing classes are the same\n",
        "assert sorted(os.listdir(train_dir)) == sorted(os.listdir(test_dir))\n",
        "\n",
        "\n",
        "print(num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAsNczks0Gj_",
        "colab_type": "text"
      },
      "source": [
        "Before importing the data into Python Numpy arrays let's have a look at the model we will be using.\n",
        "\n",
        "\n",
        "We will be demonstrating using the [mobilenetv2 network ](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html), but you can also select [VGG16](https://neurohive.io/en/popular-networks/vgg16/) network or[ InceptionV3](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202) These networks are built into keras and is easy to use through the [applications toolkit](https://keras.io/applications/).\n",
        "\n",
        "You are welcome to use a different network that is part of the applications toolkit.\n",
        "\n",
        "Notice how deep is the network below. Have a look at the summary ouput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEDupXaRBK3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select which model you would like to use here\n",
        "# Currently you can use mobilenetv2, inceptionv3 or vgg16\n",
        "model_to_use = 'mobilenetv2'\n",
        "weights= 'imagenet'\n",
        "\n",
        "model, image_shape, preprocess_input = network_library(model_to_use, num_classes, weights=weights)\n",
        "\n",
        "model.summary()\n",
        "print(image_shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UgQUqZBDNRK",
        "colab_type": "text"
      },
      "source": [
        "The reason we are defining the model before we import the data is because we are going to use a specific set of weights for our model. These weights have been found by training the network on a very large dataset of general images. This large dataset is called [ImageNet](http://www.image-net.org/).\n",
        "\n",
        "We can see above that the normal network has an input shape of (224,224,3) so in order to use the ImageNet weights we need to import each of our images at that size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KPjbb9it5CB",
        "colab_type": "text"
      },
      "source": [
        "Using a function defined above we can import our data from the directories to numpy arrays. While importing the data, we also rescale it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGBQqFhBocA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Importing training data...')\n",
        "train_data_all, train_labels_all = dir_to_array(train_dir, image_shape)\n",
        "\n",
        "print('Importing testing data...')\n",
        "test_data, test_labels = dir_to_array(test_dir, image_shape)\n",
        "\n",
        "\n",
        "\n",
        "# This line is just selecting a subset of the training data to work with.\n",
        "# It is done here so that training is faster, however normally you would not be dumping most of the training data!\n",
        "#Taking only 0.2 of training data here\n",
        "_, train_data, _, train_labels = train_test_split(train_data_all, train_labels_all, test_size=0.2, random_state=42, stratify=train_labels_all)\n",
        "\n",
        "plt.imshow(train_data[900])\n",
        "\n",
        "#train_data = train_data.astype('float32')/255.\n",
        "#test_data = test_data.astype('float32')/255.\n",
        "train_data = preprocess_input(train_data)\n",
        "test_data = preprocess_input(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K0ZRPXpuB9z",
        "colab_type": "text"
      },
      "source": [
        "As per usual, let's check the dimensions of our data to ensure that everything has gone smoothly and have a look at one of the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyD2nwxJojhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pLtVS0jDNqx",
        "colab_type": "text"
      },
      "source": [
        "Just doing some memory tidying up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9VPiFIvc1SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JRtikmNDUTm",
        "colab_type": "text"
      },
      "source": [
        "Here, we use familiar one-hot encoding for our labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjtja-SfrcK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"First 5 training labels: \", train_labels[:5]) \n",
        "\n",
        "y_train = to_categorical(train_labels, num_classes)\n",
        "y_test = to_categorical(test_labels, num_classes)\n",
        "\n",
        "print(\"First 5 training lables as one-hot encoded vectors:\\n\", y_train[:5])\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRNCvU6bDcfw",
        "colab_type": "text"
      },
      "source": [
        "Finally, all preparations are done and we can compile our model. Please note that the data preparation normally takes a big chunk of time in data science workflow. \n",
        "\n",
        "Notice a small learning rate, in general we use small learning rates in fine-tuning. In summary, we compile a model here which is MobileNetv2 and with the weigths after pre-training on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNwzh31C2Zy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieyGY9l1DysC",
        "colab_type": "text"
      },
      "source": [
        "Let's train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93XeltX8BLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_data, y_train, batch_size=16, epochs=20, verbose=True, validation_split=.1)\n",
        "loss, accuracy  = model.evaluate(test_data, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld9pNiiwFk0y",
        "colab_type": "text"
      },
      "source": [
        "And let's see the results! We do look at test set here as well. But if you compare the results of multiple settings (hyperparameter tuning) on the same dataset, you should only do in the very end!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiWcIKcvOdPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZhwJdhejUZ2",
        "colab_type": "text"
      },
      "source": [
        "We can say that this particular network would benifit from further training. \n",
        "\n",
        "Now lets compare to training the same model using randomly initialized weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNx3gQF7imjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "weights= None\n",
        "\n",
        "\n",
        "model, image_shape, preprocess_input = network_library(model_to_use, num_classes, weights=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bJ_B9Y7ixDQ",
        "colab_type": "text"
      },
      "source": [
        "Here we have set the weights to be None, this tells Keras that we want random weights. Let's compile and train using the same optimizer and learning rate as before. (we did not need to set the \"model_to_use \" again because we want to use the same model as before)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbmEV5ijivZc",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeOidM-mGRE7",
        "colab_type": "text"
      },
      "source": [
        "And training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YSAgWI4UivZu",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_data, y_train, batch_size=16, epochs=20, verbose=True, validation_split=.1)\n",
        "loss, accuracy  = model.evaluate(test_data, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABJGzq48kc7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP7lY9Kfjg5U",
        "colab_type": "text"
      },
      "source": [
        "This model initilised from random weights takes much much longer to start learning. \n",
        "\n",
        "The difference between these models is that one has weights that are known to work well for general image recognition. \n",
        "Using these \"good weights\" means our network starts with some prior knowledge taken from some other task. This is so called **fine-tuning**.\n",
        "\n",
        "Next, let's have a look at **transfer learning** where we **do not** need to conduct a long CNN training run at all.\n",
        "\n",
        "First we initialize a model using the pre-trained weights we had before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FH2TCG-jaS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_to_use ='inceptionv3' # change the model here if you like\n",
        "\n",
        "def build_extraction_model(model_to_use):\n",
        "  if model_to_use == 'vgg16':\n",
        "    extraction_model = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=image_shape, pooling='max')\n",
        "  elif model_to_use == 'inceptionv3':\n",
        "    extraction_model = applications.inception_v3.InceptionV3(include_top = False, weights='imagenet', input_shape=image_shape, pooling='avg')\n",
        "  elif model_to_use == 'mobilenetv2':\n",
        "    extraction_model = applications.mobilenet_v2.MobileNetV2(input_shape=image_shape, alpha=1.0, include_top=False, weights='imagenet', pooling='avg')\n",
        "  return extraction_model\n",
        "\n",
        "\n",
        "\n",
        "extraction_model = build_extraction_model(model_to_use)\n",
        "\n",
        "\n",
        "train_features = extraction_model.predict(train_data) # Here we convert our train_date images to features\n",
        "test_features = extraction_model.predict(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe9CP_Z1mNtr",
        "colab_type": "text"
      },
      "source": [
        "Now lets have a look at how our new features matrix looks like. We converted our 1311 trainining images into vectors of size 2048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYd3UNwNl7SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "\n",
        "print(train_features[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4FoS8Q8sy4U",
        "colab_type": "text"
      },
      "source": [
        "If we want to, we can normalize or reduce the dimensions of our feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArnCYgfpsxvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = False\n",
        "pca_reduce = False\n",
        "n_components = 2\n",
        "\n",
        "\n",
        "if normalize:\n",
        "  train_features = normalize(train_features)\n",
        "  test_features = normalize(test_features)\n",
        "\n",
        "if pca_reduce:\n",
        "  pca_transform = PCA(n_components= n_components, whiten=True)\n",
        "\n",
        "  train_features = pca_transform.fit_transform(train_features)\n",
        "  test_features = pca_transform.transform(test_features)\n",
        "\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_QghmjpmT_4",
        "colab_type": "text"
      },
      "source": [
        "So, we have used the knowlege from a general image recognition task, to transform our images into feature vectors. Let's try out a bunch of different classification techniques to classify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnwk6de9mGYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifiers = {'Logistic regression'\n",
        "               'Linear discriminant analysis':LinearDiscriminantAnalysis(),\n",
        "               'Support vector RBF':SVC(gamma='auto'),\n",
        "               'Support vector POLY': SVC(kernel='poly', gamma='auto'),\n",
        "               'Gauusian naive bayes': GaussianNB(),\n",
        "               'K-nearest neighbour':KNeighborsClassifier()}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "  clf.fit(train_features, np.squeeze(train_labels))\n",
        "  acc = clf.score(test_features, np.squeeze(test_labels))\n",
        "  print( 'The {} classifier got an accuracy of : {}'.format(clf_name, acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-1xZDTk98FD",
        "colab_type": "text"
      },
      "source": [
        "Doesn't look too bad. With much faster approach we achieved 0.65 accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj7WT9nmtXQo",
        "colab_type": "text"
      },
      "source": [
        "We could even train a different neural network to classify these new feature vectors. This is in a way similar to fine-tuning. You can think about it as adding different fully-connected \"tail\" to our base network and only training this \"tail\", not the whole body of the network =)))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsn-IX3vojC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_size = train_features.shape[1]\n",
        "\n",
        "small_network = Sequential()\n",
        "\n",
        "small_network.add(Dense(64,input_shape = (feature_size,), activation='relu'))\n",
        "small_network.add(Dense(64, activation='relu'))\n",
        "small_network.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "small_network.compile('Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = small_network.fit(train_features, y_train, batch_size=16, epochs=20, validation_split=0.1)\n",
        "loss, accuracy  = small_network.evaluate(test_features, y_test, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFarooevwgpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss_accuracy(history)\n",
        "print(f'Test loss: {loss:.3}')\n",
        "print(f'Test accuracy: {accuracy:.3}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ8mPiTSrPsm",
        "colab_type": "text"
      },
      "source": [
        "The final exercise for everyone is to try find a group of settings that works the best for them!\n",
        "\n",
        "There is a script [here ](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) that compares a bunch of different classifiers  see if you can find a network / classifier combination that works better then the what I have here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy0YkDqPrZrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}